#!/bin/bash

# Real-world integration test for KonveyN2AI using production deployment
# Tests actual workflow with questions from README.md

set -e

# Load secure test configuration (removes hard-coded URLs)
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "$SCRIPT_DIR/test-config.sh"

# Service URLs are now loaded from test-config.sh with dynamic discovery
# No hard-coded fallbacks - must be provided via environment or service discovery

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

function log_info() {
    echo -e "${BLUE}â„¹ï¸  $1${NC}"
}

function log_success() {
    echo -e "${GREEN}âœ… $1${NC}"
}

function log_warning() {
    echo -e "${YELLOW}âš ï¸  $1${NC}"
}

function log_error() {
    echo -e "${RED}âŒ $1${NC}"
}

function test_service_health() {
    local service_name=$1
    local service_url=$2
    
    log_info "Testing $service_name health..."
    
    response=$(curl -s -w "%{http_code}" "$service_url/health" -o /tmp/health_response.json)
    
    if [ "$response" = "200" ]; then
        log_success "$service_name is healthy"
        return 0
    else
        log_error "$service_name health check failed (HTTP $response)"
        return 1
    fi
}

function test_janapada_search() {
    local query=$1
    local test_name=$2
    
    log_info "Testing Janapada search: $test_name"
    
    # Test semantic search directly
    response=$(curl -s -X POST "$JANAPADA_URL/" \
        -H "Content-Type: application/json" \
        -d "{
            \"jsonrpc\": \"2.0\",
            \"method\": \"search\",
            \"params\": {
                \"query\": \"$query\",
                \"k\": 5
            },
            \"id\": \"test-search-$(date +%s)\"
        }")
    
    # Check if response contains results
    if echo "$response" | jq -e '.result.snippets | length > 0' > /dev/null 2>&1; then
        local snippet_count=$(echo "$response" | jq '.result.snippets | length')
        log_success "Janapada returned $snippet_count relevant code snippets"
        
        # Show first snippet for verification
        local first_snippet=$(echo "$response" | jq -r '.result.snippets[0].content' | head -2)
        echo "   ğŸ“„ Sample result: $first_snippet..."
        return 0
    else
        log_error "Janapada search failed or returned no results"
        echo "   ğŸ“„ Response: $(echo "$response" | jq -r '.error.message // .result')"
        return 1
    fi
}

function test_amatya_advice() {
    local role=$1
    local test_name=$2

    log_info "Testing Amatya advice generation: $test_name"

    # Test advice generation with sample question and empty chunks
    local sample_question="How can I contribute effectively to this project as a $role?"
    response=$(curl -s -X POST "$AMATYA_URL/" \
        -H "Content-Type: application/json" \
        -d "{
            \"jsonrpc\": \"2.0\",
            \"method\": \"advise\",
            \"params\": {
                \"role\": \"$role\",
                \"question\": \"$sample_question\",
                \"chunks\": []
            },
            \"id\": \"test-advice-$(date +%s)\"
        }")

    # Check if response contains advice
    if echo "$response" | jq -e '.result.advice' > /dev/null 2>&1; then
        local advice_preview=$(echo "$response" | jq -r '.result.advice' | head -1)
        log_success "Amatya generated role-based advice for $role"
        echo "   ğŸ’¡ Advice preview: $advice_preview..."
        return 0
    else
        log_error "Amatya advice generation failed"
        echo "   ğŸ“„ Response: $(echo "$response" | jq -r '.error.message // .result')"
        return 1
    fi
}

function test_full_workflow() {
    local question=$1
    local role=$2
    local test_name=$3

    log_info "Testing full workflow: $test_name"

    # Test the complete orchestrated workflow via Svami with secure authentication
    local auth_header=$(create_auth_header "bearer")
    response=$(curl -s -X POST "$SVAMI_URL/answer" \
        -H "Content-Type: application/json" \
        -H "Authorization: $auth_header" \
        -d "{
            \"question\": \"$question\",
            \"role\": \"$role\"
        }")

    # Check response status
    if echo "$response" | jq -e '.answer' > /dev/null 2>&1; then
        local answer_preview=$(echo "$response" | jq -r '.answer' | head -1)
        local sources_count=$(echo "$response" | jq '.sources | length' 2>/dev/null || echo "0")
        log_success "Full workflow completed successfully"
        echo "   ğŸ“‹ Answer preview: $answer_preview..."
        echo "   ğŸ“š Sources found: $sources_count"
        return 0
    elif echo "$response" | jq -e '.error' > /dev/null 2>&1; then
        local error_msg=$(echo "$response" | jq -r '.error')
        log_warning "Workflow returned error (expected for auth): $error_msg"
        return 0  # Auth errors are expected in production
    else
        log_error "Full workflow failed unexpectedly"
        echo "   ğŸ“„ Response: $response"
        return 1
    fi
}

function test_authenticated_workflow() {
    local question=$1
    local role=$2
    local test_name=$3
    local start_time=$(date +%s.%N)

    log_info "Testing authenticated workflow: $test_name"

    # Test with secure authentication token from configuration
    local auth_header=$(create_auth_header "bearer")
    response=$(curl -s -X POST "$SVAMI_URL/answer" \
        -H "Content-Type: application/json" \
        -H "Authorization: $auth_header" \
        -d "{
            \"question\": \"$question\",
            \"role\": \"$role\"
        }")

    local end_time=$(date +%s.%N)
    local response_time=$(echo "$end_time - $start_time" | bc -l)

    # Check response status and quality
    if echo "$response" | jq -e '.answer' > /dev/null 2>&1; then
        local answer=$(echo "$response" | jq -r '.answer')
        local answer_preview=$(echo "$answer" | head -c 100)
        local sources_count=$(echo "$response" | jq '.sources | length' 2>/dev/null || echo "0")
        local request_id=$(echo "$response" | jq -r '.request_id // "unknown"')

        # Analyze response quality
        local is_contextual=false
        local is_specific=false
        local not_generic=true

        # Check if response addresses the question (simplified contextual matching)
        # Extract key technical terms from the question
        if echo "$answer" | grep -qiE "(jwt|authentication|middleware|fastapi)" && echo "$question" | grep -qiE "(jwt|authentication|middleware)"; then
            is_contextual=true
        elif echo "$answer" | grep -qiE "(database|connection|pooling)" && echo "$question" | grep -qiE "(database|connection|pooling)"; then
            is_contextual=true
        elif echo "$answer" | grep -qiE "(rate.limit|api.endpoint)" && echo "$question" | grep -qiE "(rate.limit|api.endpoint)"; then
            is_contextual=true
        elif echo "$answer" | grep -qiE "(ci/cd|pipeline|deployment)" && echo "$question" | grep -qiE "(ci/cd|pipeline)"; then
            is_contextual=true
        elif echo "$answer" | grep -qiE "(websocket|real.time)" && echo "$question" | grep -qiE "(websocket|real.time)"; then
            is_contextual=true
        elif echo "$answer" | grep -qiE "(error.handling|async|exception)" && echo "$question" | grep -qiE "(error.handling|async)"; then
            is_contextual=true
        elif echo "$answer" | grep -qiE "(performance|optimization|latency)" && echo "$question" | grep -qiE "(performance|optimization|latency)"; then
            is_contextual=true
        elif echo "$answer" | grep -qiE "(testing|microservices)" && echo "$question" | grep -qiE "(testing|microservices)"; then
            is_contextual=true
        elif echo "$answer" | grep -qiE "(documentation|api)" && echo "$question" | grep -qiE "(documentation|api)"; then
            is_contextual=true
        elif echo "$answer" | grep -qiE "(architectural|patterns|multi.agent)" && echo "$question" | grep -qiE "(architectural|patterns|multi.agent)"; then
            is_contextual=true
        fi

        # Check for specific technical content
        if echo "$answer" | grep -qE "(implement|configure|setup|install|create|code|example)"; then
            is_specific=true
        fi

        # Check for generic onboarding language (indicates old bug)
        if echo "$answer" | grep -qiE "(welcome to the team|onboarding guide|getting started|step-by-step guidance)"; then
            not_generic=false
        fi

        # Determine test result
        if [ "$is_contextual" = true ] && [ "$is_specific" = true ] && [ "$not_generic" = true ]; then
            log_success "Authenticated workflow passed quality checks"
            echo "   â±ï¸  Response time: ${response_time}s"
            echo "   ğŸ“‹ Answer preview: $answer_preview..."
            echo "   ğŸ“š Sources found: $sources_count"
            echo "   ğŸ†” Request ID: $request_id"
            echo "   âœ… Quality: Contextual, Specific, Not Generic"
            return 0
        else
            log_warning "Authenticated workflow completed but failed quality checks"
            echo "   â±ï¸  Response time: ${response_time}s"
            echo "   ğŸ“‹ Answer preview: $answer_preview..."
            echo "   ğŸ“š Sources found: $sources_count"
            echo "   ğŸ†” Request ID: $request_id"
            echo "   âš ï¸  Quality issues: Contextual=$is_contextual, Specific=$is_specific, NotGeneric=$not_generic"
            return 1
        fi
    elif echo "$response" | jq -e '.error' > /dev/null 2>&1; then
        local error_msg=$(echo "$response" | jq -r '.error')
        log_error "Authenticated workflow failed with error: $error_msg"
        echo "   â±ï¸  Response time: ${response_time}s"
        return 1
    else
        log_error "Authenticated workflow failed unexpectedly"
        echo "   â±ï¸  Response time: ${response_time}s"
        echo "   ğŸ“„ Response: $response"
        return 1
    fi
}

function evaluate_agent_performance() {
    # Comprehensive agent evaluation against industry best practices.
    # Evaluates: Response relevance, contextual understanding, role-based customization,
    # error handling, performance benchmarks, and security compliance.

    log_info "Starting comprehensive agent evaluation..."

    local evaluation_success=0
    local total_score=0
    local max_score=60  # 6 categories Ã— 10 points each

    # Evaluation Categories
    local relevance_score=0
    local contextual_score=0
    local role_based_score=0
    local error_handling_score=0
    local performance_score=0
    local security_score=0

    echo "ğŸ“Š Evaluation Categories:"
    echo "  1. Response Relevance and Accuracy"
    echo "  2. Contextual Understanding"
    echo "  3. Role-Based Customization"
    echo "  4. Error Handling and Graceful Degradation"
    echo "  5. Performance Benchmarks"
    echo "  6. Security Compliance"
    echo ""

    # Category 1: Response Relevance and Accuracy (10 points)
    echo "ğŸ¯ Category 1: Response Relevance and Accuracy"
    echo "---------------------------------------------"

    relevance_score=$(evaluate_response_relevance 2>/dev/null | tail -1)
    total_score=$((total_score + relevance_score))

    if [ $relevance_score -ge 8 ]; then
        log_success "Response relevance: EXCELLENT ($relevance_score/10)"
    elif [ $relevance_score -ge 6 ]; then
        log_warning "Response relevance: GOOD ($relevance_score/10)"
    else
        log_error "Response relevance: NEEDS IMPROVEMENT ($relevance_score/10)"
        evaluation_success=1
    fi
    echo ""

    # Category 2: Contextual Understanding (10 points)
    echo "ğŸ§  Category 2: Contextual Understanding"
    echo "-------------------------------------"

    contextual_score=$(evaluate_contextual_understanding 2>/dev/null | tail -1)
    total_score=$((total_score + contextual_score))

    if [ $contextual_score -ge 8 ]; then
        log_success "Contextual understanding: EXCELLENT ($contextual_score/10)"
    elif [ $contextual_score -ge 6 ]; then
        log_warning "Contextual understanding: GOOD ($contextual_score/10)"
    else
        log_error "Contextual understanding: NEEDS IMPROVEMENT ($contextual_score/10)"
        evaluation_success=1
    fi
    echo ""

    # Category 3: Role-Based Customization (10 points)
    echo "ğŸ‘¥ Category 3: Role-Based Customization"
    echo "--------------------------------------"

    role_based_score=$(evaluate_role_based_customization 2>/dev/null | tail -1)
    total_score=$((total_score + role_based_score))

    if [ $role_based_score -ge 8 ]; then
        log_success "Role-based customization: EXCELLENT ($role_based_score/10)"
    elif [ $role_based_score -ge 6 ]; then
        log_warning "Role-based customization: GOOD ($role_based_score/10)"
    else
        log_error "Role-based customization: NEEDS IMPROVEMENT ($role_based_score/10)"
        evaluation_success=1
    fi
    echo ""

    # Category 4: Error Handling and Graceful Degradation (10 points)
    echo "ğŸ›¡ï¸ Category 4: Error Handling and Graceful Degradation"
    echo "----------------------------------------------------"

    error_handling_score=$(evaluate_error_handling 2>/dev/null | tail -1)
    total_score=$((total_score + error_handling_score))

    if [ $error_handling_score -ge 8 ]; then
        log_success "Error handling: EXCELLENT ($error_handling_score/10)"
    elif [ $error_handling_score -ge 6 ]; then
        log_warning "Error handling: GOOD ($error_handling_score/10)"
    else
        log_error "Error handling: NEEDS IMPROVEMENT ($error_handling_score/10)"
        evaluation_success=1
    fi
    echo ""

    # Category 5: Performance Benchmarks (10 points)
    echo "âš¡ Category 5: Performance Benchmarks"
    echo "-----------------------------------"

    performance_score=$(evaluate_performance_benchmarks 2>/dev/null | tail -1)
    total_score=$((total_score + performance_score))

    if [ $performance_score -ge 8 ]; then
        log_success "Performance: EXCELLENT ($performance_score/10)"
    elif [ $performance_score -ge 6 ]; then
        log_warning "Performance: GOOD ($performance_score/10)"
    else
        log_error "Performance: NEEDS IMPROVEMENT ($performance_score/10)"
        evaluation_success=1
    fi
    echo ""

    # Category 6: Security Compliance (10 points)
    echo "ğŸ”’ Category 6: Security Compliance"
    echo "---------------------------------"

    security_score=$(evaluate_security_compliance 2>/dev/null | tail -1)
    total_score=$((total_score + security_score))

    if [ $security_score -ge 8 ]; then
        log_success "Security compliance: EXCELLENT ($security_score/10)"
    elif [ $security_score -ge 6 ]; then
        log_warning "Security compliance: GOOD ($security_score/10)"
    else
        log_error "Security compliance: NEEDS IMPROVEMENT ($security_score/10)"
        evaluation_success=1
    fi
    echo ""

    # Final Evaluation Summary
    echo "ğŸ“Š AGENT EVALUATION SUMMARY"
    echo "=========================="
    local percentage=$((total_score * 100 / max_score))
    echo "   Total Score: $total_score/$max_score ($percentage%)"
    echo ""
    echo "   ğŸ“ˆ Category Breakdown:"
    echo "   â€¢ Response Relevance: $relevance_score/10"
    echo "   â€¢ Contextual Understanding: $contextual_score/10"
    echo "   â€¢ Role-Based Customization: $role_based_score/10"
    echo "   â€¢ Error Handling: $error_handling_score/10"
    echo "   â€¢ Performance: $performance_score/10"
    echo "   â€¢ Security Compliance: $security_score/10"
    echo ""

    if [ $percentage -ge 80 ]; then
        log_success "ğŸ† AGENT EVALUATION: PRODUCTION READY ($percentage%)"
        echo "   âœ… Meets industry best practices for production AI systems"
    elif [ $percentage -ge 70 ]; then
        log_warning "âš ï¸ AGENT EVALUATION: GOOD WITH IMPROVEMENTS NEEDED ($percentage%)"
        echo "   ğŸ”§ Some areas need optimization before full production deployment"
    else
        log_error "âŒ AGENT EVALUATION: SIGNIFICANT IMPROVEMENTS REQUIRED ($percentage%)"
        echo "   ğŸš¨ Major issues must be addressed before production deployment"
        evaluation_success=1
    fi

    return $evaluation_success
}

# =============================================================================
# EVALUATION FUNCTIONS - Mathematical Scoring (0-10 scale)
# =============================================================================

function evaluate_response_relevance() {
    # Evaluate response relevance and accuracy using quantitative metrics.
    # Scoring: 0-10 based on keyword matching, technical accuracy, and completeness.

    local total_relevance=0
    local test_count=0
    local auth_header=$(create_auth_header "bearer")

    # Test diverse technical questions for relevance
    local test_queries=(
        "How do I implement JWT authentication middleware in FastAPI?|jwt|authentication|middleware|fastapi"
        "What are the best practices for database connection pooling?|database|connection|pooling|best.practices"
        "How can I implement rate limiting for API endpoints?|rate.limit|api|endpoint|implement"
        "What testing strategies work best for microservices?|testing|strategies|microservices|best"
    )

    echo "   ğŸ” Testing response relevance across technical domains..." >&2

    for query_data in "${test_queries[@]}"; do
        IFS='|' read -r question keywords <<< "$query_data"

        # Get response from system
        local response=$(curl -s -X POST "$SVAMI_URL/answer" \
            -H "Content-Type: application/json" \
            -H "Authorization: $auth_header" \
            -d "{\"question\": \"$question\", \"role\": \"backend_developer\"}" | jq -r '.answer // ""')

        if [ -n "$response" ] && [ "$response" != "null" ]; then
            # Calculate keyword relevance score (0-10)
            local keyword_matches=0
            local total_keywords=0

            IFS='|' read -ra KEYWORD_ARRAY <<< "$keywords"
            for keyword in "${KEYWORD_ARRAY[@]}"; do
                total_keywords=$((total_keywords + 1))
                if echo "$response" | grep -qiE "$keyword"; then
                    keyword_matches=$((keyword_matches + 1))
                fi
            done

            # Calculate relevance percentage and convert to 0-10 scale
            local relevance_percentage=$((keyword_matches * 100 / total_keywords))
            local question_score=$((relevance_percentage / 10))

            total_relevance=$((total_relevance + question_score))
            test_count=$((test_count + 1))

            echo "     â€¢ Question relevance: $question_score/10 ($relevance_percentage% keyword match)" >&2
        fi
    done

    # Calculate average relevance score
    local avg_relevance=0
    if [ $test_count -gt 0 ]; then
        avg_relevance=$((total_relevance / test_count))
    fi

    echo "   ğŸ“Š Average relevance score: $avg_relevance/10" >&2
    echo $avg_relevance
}

function evaluate_contextual_understanding() {
    # Evaluate contextual understanding using response coherence and domain expertise.
    # Scoring: 0-10 based on context awareness, technical depth, and logical flow.

    echo "   ğŸ§  Testing contextual understanding and domain expertise..." >&2

    local context_score=0
    local technical_depth_score=0
    local coherence_score=0
    local auth_header=$(create_auth_header "bearer")

    # Test context awareness with complex technical question
    local complex_response=$(curl -s -X POST "$SVAMI_URL/answer" \
        -H "Content-Type: application/json" \
        -H "Authorization: $auth_header" \
        -d '{"question": "How do I implement secure JWT authentication with refresh tokens and role-based access control?", "role": "security_engineer"}' | jq -r '.answer // ""')

    if [ -n "$complex_response" ] && [ "$complex_response" != "null" ]; then
        # Context Awareness (0-4 points)
        local context_indicators=("jwt" "refresh.token" "role.based" "access.control" "security")
        local context_matches=0
        for indicator in "${context_indicators[@]}"; do
            if echo "$complex_response" | grep -qiE "$indicator"; then
                context_matches=$((context_matches + 1))
            fi
        done
        context_score=$((context_matches * 4 / 5))  # Scale to 0-4

        # Technical Depth (0-3 points)
        local depth_indicators=("implement" "configure" "example" "code" "step")
        local depth_matches=0
        for indicator in "${depth_indicators[@]}"; do
            if echo "$complex_response" | grep -qiE "$indicator"; then
                depth_matches=$((depth_matches + 1))
            fi
        done
        technical_depth_score=$((depth_matches * 3 / 5))  # Scale to 0-3

        # Response Coherence (0-3 points)
        local response_length=${#complex_response}
        if [ $response_length -gt 500 ]; then
            coherence_score=3  # Detailed response
        elif [ $response_length -gt 200 ]; then
            coherence_score=2  # Moderate response
        elif [ $response_length -gt 50 ]; then
            coherence_score=1  # Brief response
        else
            coherence_score=0  # Too brief
        fi
    fi

    local total_contextual=$((context_score + technical_depth_score + coherence_score))

    echo "     â€¢ Context awareness: $context_score/4" >&2
    echo "     â€¢ Technical depth: $technical_depth_score/3" >&2
    echo "     â€¢ Response coherence: $coherence_score/3" >&2
    echo "   ğŸ“Š Total contextual understanding: $total_contextual/10" >&2

    echo $total_contextual
}

function evaluate_role_based_customization() {
    # Evaluate role-based response customization and expertise differentiation.
    # Scoring: 0-10 based on role-specific terminology, perspective, and recommendations.

    echo "   ğŸ‘¥ Testing role-based customization across different personas..." >&2

    local role_differentiation_score=0
    local terminology_score=0
    local perspective_score=0

    # Test same question across different roles
    local test_question="How do I implement authentication in this system?"

    local auth_header=$(create_auth_header "bearer")

    # Backend Developer Response
    local backend_response=$(curl -s -X POST "$SVAMI_URL/answer" \
        -H "Content-Type: application/json" \
        -H "Authorization: $auth_header" \
        -d "{\"question\": \"$test_question\", \"role\": \"backend_developer\"}" | jq -r '.answer // ""')

    # Security Engineer Response
    local security_response=$(curl -s -X POST "$SVAMI_URL/answer" \
        -H "Content-Type: application/json" \
        -H "Authorization: $auth_header" \
        -d "{\"question\": \"$test_question\", \"role\": \"security_engineer\"}" | jq -r '.answer // ""')

    if [ -n "$backend_response" ] && [ -n "$security_response" ] && [ "$backend_response" != "null" ] && [ "$security_response" != "null" ]; then
        # Role Differentiation (0-4 points) - Check for different approaches
        local backend_terms=("api" "endpoint" "middleware" "framework" "implementation")
        local security_terms=("threat" "vulnerability" "encryption" "secure" "compliance")

        local backend_matches=0
        local security_matches=0

        for term in "${backend_terms[@]}"; do
            if echo "$backend_response" | grep -qiE "$term"; then
                backend_matches=$((backend_matches + 1))
            fi
        done

        for term in "${security_terms[@]}"; do
            if echo "$security_response" | grep -qiE "$term"; then
                security_matches=$((security_matches + 1))
            fi
        done

        role_differentiation_score=$(((backend_matches + security_matches) * 4 / 10))  # Scale to 0-4

        # Terminology Appropriateness (0-3 points)
        if echo "$backend_response" | grep -qiE "(backend|api|implementation)" && echo "$security_response" | grep -qiE "(security|secure|threat)"; then
            terminology_score=3
        elif echo "$backend_response" | grep -qiE "(backend|api)" || echo "$security_response" | grep -qiE "(security|secure)"; then
            terminology_score=2
        else
            terminology_score=1
        fi

        # Perspective Differentiation (0-3 points)
        local response_similarity=$(echo "$backend_response" | head -c 100)
        local security_similarity=$(echo "$security_response" | head -c 100)

        if [ "$response_similarity" != "$security_similarity" ]; then
            perspective_score=3  # Different perspectives
        else
            perspective_score=1  # Similar responses
        fi
    fi

    local total_role_based=$((role_differentiation_score + terminology_score + perspective_score))

    echo "     â€¢ Role differentiation: $role_differentiation_score/4" >&2
    echo "     â€¢ Terminology appropriateness: $terminology_score/3" >&2
    echo "     â€¢ Perspective variation: $perspective_score/3" >&2
    echo "   ğŸ“Š Total role-based customization: $total_role_based/10" >&2

    echo $total_role_based
}

function evaluate_error_handling() {
    # Evaluate error handling and graceful degradation capabilities.
    # Scoring: 0-10 based on authentication handling, malformed requests, and service resilience.

    echo "   ğŸ›¡ï¸ Testing error handling and graceful degradation..." >&2

    local auth_handling_score=0
    local malformed_request_score=0
    local service_resilience_score=0
    local auth_header=$(create_auth_header "bearer")

    # Test 1: Authentication Error Handling (0-4 points)
    local unauth_response=$(curl -s -X POST "$SVAMI_URL/answer" \
        -H "Content-Type: application/json" \
        -d '{"question": "test", "role": "backend_developer"}')

    if echo "$unauth_response" | grep -qiE "(authentication|auth|unauthorized|401)"; then
        auth_handling_score=4  # Proper auth error
        echo "     â€¢ Authentication errors: HANDLED CORRECTLY" >&2
    else
        auth_handling_score=2  # Some handling
        echo "     â€¢ Authentication errors: PARTIALLY HANDLED" >&2
    fi

    # Test 2: Malformed Request Handling (0-3 points)
    local malformed_response=$(curl -s -X POST "$SVAMI_URL/answer" \
        -H "Content-Type: application/json" \
        -H "Authorization: $auth_header" \
        -d '{"invalid": "json", "structure": true}')

    if echo "$malformed_response" | grep -qiE "(error|invalid|bad.request|400)"; then
        malformed_request_score=3  # Proper error handling
        echo "     â€¢ Malformed requests: HANDLED CORRECTLY" >&2
    elif [ -n "$malformed_response" ]; then
        malformed_request_score=2  # Some response
        echo "     â€¢ Malformed requests: PARTIALLY HANDLED" >&2
    else
        malformed_request_score=1  # No response
        echo "     â€¢ Malformed requests: MINIMAL HANDLING" >&2
    fi

    # Test 3: Service Resilience (0-3 points)
    # Test with empty question
    local empty_response=$(curl -s -X POST "$SVAMI_URL/answer" \
        -H "Content-Type: application/json" \
        -H "Authorization: $auth_header" \
        -d '{"question": "", "role": "backend_developer"}')

    if [ -n "$empty_response" ] && ! echo "$empty_response" | grep -qiE "(error|null)"; then
        service_resilience_score=3  # Handles edge cases
        echo "     â€¢ Edge case handling: ROBUST" >&2
    elif [ -n "$empty_response" ]; then
        service_resilience_score=2  # Some handling
        echo "     â€¢ Edge case handling: ADEQUATE" >&2
    else
        service_resilience_score=1  # Minimal handling
        echo "     â€¢ Edge case handling: BASIC" >&2
    fi

    local total_error_handling=$((auth_handling_score + malformed_request_score + service_resilience_score))

    echo "   ğŸ“Š Total error handling score: $total_error_handling/10" >&2

    echo $total_error_handling
}

function evaluate_performance_benchmarks() {
    # Evaluate performance benchmarks against industry standards.
    # Scoring: 0-10 based on response times, throughput, and consistency.

    echo "   âš¡ Testing performance benchmarks..." >&2

    local response_time_score=0
    local consistency_score=0
    local throughput_score=0
    local auth_header=$(create_auth_header "bearer")

    # Test 1: Response Time Performance (0-4 points)
    local total_time=0
    local test_count=5
    local fast_responses=0

    echo "     â€¢ Measuring response times across $test_count requests..." >&2

    for i in $(seq 1 $test_count); do
        local start_time=$(date +%s.%N)
        curl -s -X POST "$SVAMI_URL/answer" \
            -H "Content-Type: application/json" \
            -H "Authorization: $auth_header" \
            -d '{"question": "What is JWT authentication?", "role": "backend_developer"}' > /dev/null
        local end_time=$(date +%s.%N)
        local response_time=$(echo "$end_time - $start_time" | bc -l)
        total_time=$(echo "$total_time + $response_time" | bc -l)

        # Count fast responses (<3 seconds)
        if (( $(echo "$response_time < 3.0" | bc -l) )); then
            fast_responses=$((fast_responses + 1))
        fi
    done

    local avg_time=$(echo "scale=2; $total_time / $test_count" | bc -l)

    # Score based on average response time
    if (( $(echo "$avg_time < 2.0" | bc -l) )); then
        response_time_score=4  # Excellent (<2s)
    elif (( $(echo "$avg_time < 3.0" | bc -l) )); then
        response_time_score=3  # Good (<3s)
    elif (( $(echo "$avg_time < 5.0" | bc -l) )); then
        response_time_score=2  # Acceptable (<5s)
    else
        response_time_score=1  # Slow (>5s)
    fi

    echo "     â€¢ Average response time: ${avg_time}s" >&2
    echo "     â€¢ Fast responses (<3s): $fast_responses/$test_count" >&2

    # Test 2: Consistency Score (0-3 points)
    consistency_score=$((fast_responses * 3 / test_count))

    # Test 3: Throughput Score (0-3 points) - Based on successful responses
    if [ $fast_responses -eq $test_count ]; then
        throughput_score=3  # All responses successful and fast
    elif [ $fast_responses -gt $((test_count / 2)) ]; then
        throughput_score=2  # Most responses successful
    else
        throughput_score=1  # Some responses successful
    fi

    local total_performance=$((response_time_score + consistency_score + throughput_score))

    echo "     â€¢ Response time quality: $response_time_score/4" >&2
    echo "     â€¢ Consistency: $consistency_score/3" >&2
    echo "     â€¢ Throughput: $throughput_score/3" >&2
    echo "   ğŸ“Š Total performance score: $total_performance/10" >&2

    echo $total_performance
}

function evaluate_security_compliance() {
    # Evaluate security compliance and authentication mechanisms.
    # Scoring: 0-10 based on authentication enforcement, secure headers, and data protection.

    echo "   ğŸ”’ Testing security compliance..." >&2

    local auth_enforcement_score=0
    local secure_headers_score=0
    local data_protection_score=0
    local auth_header=$(create_auth_header "bearer")

    # Test 1: Authentication Enforcement (0-4 points)
    echo "     â€¢ Testing authentication enforcement..." >&2

    # Test without auth token
    local unauth_status=$(curl -s -o /dev/null -w "%{http_code}" -X POST "$SVAMI_URL/answer" \
        -H "Content-Type: application/json" \
        -d '{"question": "test", "role": "backend_developer"}')

    # Test with invalid token
    local invalid_auth_status=$(curl -s -o /dev/null -w "%{http_code}" -X POST "$SVAMI_URL/answer" \
        -H "Content-Type: application/json" \
        -H "Authorization: Bearer invalid-token" \
        -d '{"question": "test", "role": "backend_developer"}')

    # Test with valid token
    local valid_auth_status=$(curl -s -o /dev/null -w "%{http_code}" -X POST "$SVAMI_URL/answer" \
        -H "Content-Type: application/json" \
        -H "Authorization: $auth_header" \
        -d '{"question": "test", "role": "backend_developer"}')

    # Score authentication enforcement
    if [ "$unauth_status" != "200" ] && [ "$invalid_auth_status" != "200" ] && [ "$valid_auth_status" = "200" ]; then
        auth_enforcement_score=4  # Perfect auth enforcement
        echo "       âœ… Authentication properly enforced" >&2
    elif [ "$valid_auth_status" = "200" ]; then
        auth_enforcement_score=2  # Valid auth works, but may not block invalid
        echo "       âš ï¸ Authentication partially enforced" >&2
    else
        auth_enforcement_score=1  # Minimal auth
        echo "       âŒ Authentication issues detected" >&2
    fi

    # Test 2: Secure Headers (0-3 points)
    echo "     â€¢ Testing security headers..." >&2
    local headers=$(curl -s -I "$SVAMI_URL/health")

    local header_count=0
    if echo "$headers" | grep -qi "content-type"; then
        header_count=$((header_count + 1))
    fi
    if echo "$headers" | grep -qi "server"; then
        header_count=$((header_count + 1))
    fi
    if echo "$headers" | grep -qi "x-"; then
        header_count=$((header_count + 1))
    fi

    secure_headers_score=$((header_count))
    if [ $secure_headers_score -gt 3 ]; then
        secure_headers_score=3
    fi

    echo "       â€¢ Security headers present: $header_count" >&2

    # Test 3: Data Protection (0-3 points)
    echo "     â€¢ Testing data protection..." >&2

    # Test for sensitive data exposure
    local response_content=$(curl -s -X POST "$SVAMI_URL/answer" \
        -H "Content-Type: application/json" \
        -H "Authorization: $auth_header" \
        -d '{"question": "Show me API keys or secrets", "role": "backend_developer"}')

    # Check if response contains potential sensitive data patterns
    if echo "$response_content" | grep -qiE "(api.key|secret|password|token.*[a-zA-Z0-9]{20,})"; then
        data_protection_score=1  # Potential data exposure
        echo "       âš ï¸ Potential sensitive data patterns detected" >&2
    else
        data_protection_score=3  # No obvious sensitive data
        echo "       âœ… No sensitive data exposure detected" >&2
    fi

    local total_security=$((auth_enforcement_score + secure_headers_score + data_protection_score))

    echo "     â€¢ Authentication enforcement: $auth_enforcement_score/4" >&2
    echo "     â€¢ Secure headers: $secure_headers_score/3" >&2
    echo "     â€¢ Data protection: $data_protection_score/3" >&2
    echo "   ğŸ“Š Total security compliance: $total_security/10" >&2

    echo $total_security
}

# =============================================================================
# TODO: FUTURE ENHANCEMENT - LLM-as-a-Judge Evaluation Framework
# =============================================================================
#
# NEXT PHASE: Advanced AI-Powered Evaluation System
# ================================================
#
# The current evaluation system uses keyword-based and mathematical approaches
# which provide basic quantitative assessment. The next evolution should implement
# sophisticated LLM-as-a-Judge methodologies for more nuanced evaluation.
#
# ğŸ¯ PLANNED ENHANCEMENTS:
#
# 1. LLM-as-a-Judge Evaluation:
#    - Replace keyword-based quality assessment with AI-powered evaluation
#    - Implement GPT-4 or Claude-3.5-Sonnet as evaluation judges
#    - Use structured prompts for consistent scoring across dimensions
#    - Framework: G-Eval methodology with Likert scale scoring (1-5)
#
# 2. Advanced Relevance Scoring:
#    - Semantic similarity using sentence-transformers or OpenAI embeddings
#    - Cosine similarity between question embeddings and response embeddings
#    - Replace simple keyword matching with vector-based relevance scoring
#    - Target: >0.8 semantic similarity for high relevance
#
# 3. Response Quality Metrics:
#    - BLEU scores for response quality against reference answers
#    - Semantic coherence analysis using transformer models
#    - Factual accuracy checking against knowledge bases
#    - Hallucination detection using consistency checking
#
# 4. Comparative Analysis:
#    - Benchmark against industry-standard AI assistants (GPT-4, Claude, etc.)
#    - A/B testing framework for response quality comparison
#    - Human evaluation integration for ground truth establishment
#    - Performance parity analysis with leading conversational AI systems
#
# 5. Multi-dimensional Scoring:
#    - Helpfulness: How well does the response solve the user's problem?
#    - Harmlessness: Does the response avoid harmful or inappropriate content?
#    - Honesty: Does the response acknowledge limitations and uncertainties?
#    - Technical Accuracy: Are the technical details correct and up-to-date?
#
# ğŸ”§ IMPLEMENTATION ROADMAP:
#
# Phase 1: LLM Judge Integration
# - Add OpenAI/Anthropic API integration for evaluation
# - Create structured evaluation prompts for each dimension
# - Implement async evaluation for performance
# - Cost optimization with evaluation sampling
#
# Phase 2: Semantic Analysis
# - Integrate sentence-transformers for embedding generation
# - Implement vector similarity calculations
# - Add semantic coherence scoring
# - Create relevance threshold tuning system
#
# Phase 3: Comparative Benchmarking
# - Build reference response database
# - Implement A/B testing framework
# - Add human evaluation interface
# - Create performance dashboards
#
# Phase 4: Production Integration
# - Real-time evaluation pipeline
# - Automated quality monitoring
# - Alert system for quality degradation
# - Continuous improvement feedback loop
#
# ğŸ“‹ EVALUATION CRITERIA FRAMEWORKS:
#
# G-Eval Framework Implementation:
# - Coherence: Logical flow and structure (1-5 scale)
# - Consistency: Internal consistency and factual alignment (1-5 scale)
# - Fluency: Language quality and readability (1-5 scale)
# - Relevance: Alignment with user query and context (1-5 scale)
#
# LLM-as-a-Judge Prompt Template:
# ```
# You are an expert evaluator assessing AI assistant responses for technical accuracy and helpfulness.
#
# Question: {user_question}
# Response: {ai_response}
# Role Context: {user_role}
#
# Evaluate the response on the following dimensions (1-5 scale):
# 1. Technical Accuracy: Are the technical details correct?
# 2. Relevance: Does it directly address the question?
# 3. Completeness: Does it provide sufficient detail?
# 4. Clarity: Is it well-structured and understandable?
# 5. Actionability: Can the user implement the guidance?
#
# Provide scores and brief justification for each dimension.
# ```
#
# ğŸ¯ SUCCESS METRICS:
# - Semantic similarity scores >0.85 for relevance
# - LLM judge scores >4.0/5.0 across all dimensions
# - Human evaluation agreement >80% with LLM judges
# - Response quality parity with GPT-4 baseline
# - Sub-3-second evaluation latency for real-time monitoring
#
# ğŸ’¡ RESEARCH REFERENCES:
# - G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment
# - LLM-as-a-Judge: Evaluating LLM-as-a-judge with MT-bench and Chatbot Arena
# - Constitutional AI: Harmlessness from AI Feedback
# - Self-Refine: Iterative Refinement with Self-Feedback
#
# =============================================================================

function run_comprehensive_integration_tests() {
    local test_phase=${1:-"all"}

    echo "ğŸ§ª KonveyN2AI Real-World Integration Testing"
    echo "============================================="
    echo "ğŸ“‹ Test Phase: $test_phase"
    echo ""
    echo "ğŸŒ Production Services:"
    echo "  ğŸ¼ Svami Orchestrator: $SVAMI_URL"
    echo "  ğŸ—„ï¸  Janapada Memory: $JANAPADA_URL"
    echo "  ğŸ­ Amatya Role Prompter: $AMATYA_URL"
    echo ""

    local overall_success=true

    # Test result tracking
    local total_tests=0
    local passed_tests=0
    local failed_tests=0
    local error_tests=0
    local auth_errors=0
    local quality_failures=0
    local api_errors=0
    local script_errors=0
    
    # Phase 1: Health Checks (skip if only testing user question fix)
    if [ "$test_phase" = "all" ] || [ "$test_phase" = "health" ]; then
        echo "ğŸ“Š Phase 1: Production Health Verification"
        echo "----------------------------------------"

        test_service_health "Svami Orchestrator" "$SVAMI_URL" || overall_success=false
        test_service_health "Janapada Memory" "$JANAPADA_URL" || overall_success=false
        test_service_health "Amatya Role Prompter" "$AMATYA_URL" || overall_success=false
        echo ""
    fi
    
    # Phase 2: Janapada Memory Tests (from README examples)
    echo "ğŸ” Phase 2: Janapada Semantic Search Tests"
    echo "-----------------------------------------"
    
    test_janapada_search "authentication middleware implementation" "Authentication Query" || overall_success=false
    test_janapada_search "FastAPI security headers CORS protection" "Security Implementation" || overall_success=false
    test_janapada_search "JWT token validation bearer authentication" "JWT Authentication" || overall_success=false
    test_janapada_search "database connection pooling SQLAlchemy" "Database Patterns" || overall_success=false
    test_janapada_search "error handling exception middleware" "Error Handling" || overall_success=false
    echo ""
    
    # Phase 3: Amatya Role Prompter Tests (from README examples)
    echo "ğŸ­ Phase 3: Amatya Role-Based Advice Tests"
    echo "----------------------------------------"
    
    test_amatya_advice "backend_developer" "Backend Developer Role" || overall_success=false
    test_amatya_advice "security_engineer" "Security Engineer Role" || overall_success=false
    test_amatya_advice "devops_specialist" "DevOps Specialist Role" || overall_success=false
    test_amatya_advice "frontend_developer" "Frontend Developer Role" || overall_success=false
    echo ""
    
    # Phase 4: End-to-End Workflow Tests (from README examples)
    echo "ğŸ¯ Phase 4: Complete Multi-Agent Workflow Tests"
    echo "----------------------------------------------"
    
    test_full_workflow "How do I implement authentication middleware?" "backend_developer" "Authentication Workflow" || overall_success=false

    # Phase 5: User Question Fix Validation Tests (NEW)
    if [ "$test_phase" = "all" ] || [ "$test_phase" = "user-question-fix" ] || [ "$test_phase" = "authenticated-only" ]; then
        echo "ğŸ”§ Phase 5: User Question Fix Validation Tests"
        echo "--------------------------------------------"
        echo "   Testing that user questions are properly passed to AI services"
        echo "   and responses are contextual rather than generic onboarding guides"
        echo ""

    # Test diverse questions with authentication to validate the fix
    test_authenticated_workflow "How do I implement JWT authentication middleware in FastAPI?" "backend_developer" "JWT Auth Implementation" || overall_success=false
    test_authenticated_workflow "What are the best practices for database connection pooling?" "backend_developer" "Database Connection Pooling" || overall_success=false
    test_authenticated_workflow "How can I implement rate limiting for API endpoints?" "security_engineer" "API Rate Limiting" || overall_success=false
    test_authenticated_workflow "What's the best way to set up CI/CD pipelines for this project?" "devops_engineer" "CI/CD Pipeline Setup" || overall_success=false
    test_authenticated_workflow "How do I implement real-time WebSocket connections?" "frontend_developer" "WebSocket Implementation" || overall_success=false
    test_authenticated_workflow "What error handling patterns should I use for async operations?" "backend_developer" "Async Error Handling" || overall_success=false
    test_authenticated_workflow "How can I optimize API response times and reduce latency?" "backend_developer" "Performance Optimization" || overall_success=false
    test_authenticated_workflow "What testing strategies work best for microservices?" "qa_engineer" "Microservices Testing" || overall_success=false
    test_authenticated_workflow "How should I structure API documentation for this project?" "technical_writer" "API Documentation" || overall_success=false
        test_authenticated_workflow "What architectural patterns are used in this multi-agent system?" "backend_developer" "Architecture Patterns" || overall_success=false
        echo ""
    fi
    test_full_workflow "How do I implement secure authentication middleware for a FastAPI application?" "backend_developer" "FastAPI Security Workflow" || overall_success=false
    test_full_workflow "What are the best practices for JWT token validation?" "security_engineer" "Security Best Practices" || overall_success=false
    test_full_workflow "How do I set up CORS and security headers?" "devops_specialist" "DevOps Security Setup" || overall_success=false
    echo ""
    
    # Phase 5: Performance and Integration Analysis
    echo "âš¡ Phase 5: Performance Analysis"
    echo "------------------------------"
    
    # Test response times
    log_info "Measuring response times..."

    start_time=$(date +%s)
    curl -s "$JANAPADA_URL/health" > /dev/null
    janapada_time=$(($(date +%s) - start_time))

    start_time=$(date +%s)
    curl -s "$AMATYA_URL/health" > /dev/null
    amatya_time=$(($(date +%s) - start_time))

    start_time=$(date +%s)
    curl -s "$SVAMI_URL/health" > /dev/null
    svami_time=$(($(date +%s) - start_time))
    
    echo "   ğŸ—„ï¸  Janapada response time: ${janapada_time}s"
    echo "   ğŸ­ Amatya response time: ${amatya_time}s"
    echo "   ğŸ¼ Svami response time: ${svami_time}s"
    echo ""

    # Phase 6: Agent Evaluation Against Industry Best Practices
    if [ "$test_phase" = "all" ] || [ "$test_phase" = "agent-evaluation" ]; then
        echo "ğŸ† Phase 6: Agent Evaluation - Industry Best Practices"
        echo "===================================================="
        echo "   Evaluating system against production AI standards"
        echo ""

        evaluate_agent_performance || overall_success=false
        echo ""
    fi
    
    # Final Results Summary
    echo "ğŸ¯ Integration Test Results Summary"
    echo "=================================="
    
    if [ "$overall_success" = true ]; then
        log_success "ğŸ† ALL INTEGRATION TESTS PASSED!"
        echo ""
        echo "âœ… Production Deployment Status: FULLY OPERATIONAL"
        echo "âœ… Multi-Agent Workflow: WORKING"
        echo "âœ… Semantic Search: FUNCTIONAL"
        echo "âœ… Role-Based AI: GENERATING ADVICE"
        echo "âœ… Service Integration: COMPLETE"
        echo "âœ… User Question Fix: VALIDATED"
        echo ""
        echo "ğŸ”§ Fix Validation Results:"
        echo "   â€¢ User questions properly passed to AI services"
        echo "   â€¢ Responses are contextual and specific"
        echo "   â€¢ No generic onboarding templates detected"
        echo "   â€¢ Gemini API integration working correctly"
        echo "   â€¢ Response quality meets production standards"
        echo ""
        echo "ğŸš€ KonveyN2AI is ready for production use!"
        echo ""
        echo "ğŸ“‹ Next Steps:"
        echo "  1. Set up custom domain (optional)"
        echo "  2. Configure load balancing for scale"
        echo "  3. Set up production monitoring alerts"
        echo "  4. Update client applications with production URLs"
        return 0
    else
        log_error "âŒ Some integration tests failed"
        echo ""
        echo "ğŸ”§ Troubleshooting:"
        echo "  1. Check service logs: gcloud logging read"
        echo "  2. Verify API keys and permissions"
        echo "  3. Confirm Google Cloud services are enabled"
        echo "  4. Test individual service endpoints"
        return 1
    fi
}

# Execute comprehensive integration tests with optional phase parameter
run_comprehensive_integration_tests "$1"