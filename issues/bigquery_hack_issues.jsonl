{"title": "M1: Set up BigQuery dataset and replace vector index", "body": "### Overview\nCreate the BigQuery dataset and baseline configuration needed to store structured and vectorized metadata for our code artifacts. Replace the existing Vertex‑based vector store in KonveyN2AI with a BigQuery vector table and update configuration variables accordingly.\n\n### Tasks\n1. Create a new BigQuery dataset (e.g., `semantic_gap_detector`) with tables for raw metadata (`source_metadata`), vector embeddings (`source_embeddings`), and analysis outputs (`gap_metrics`).\n2. Define schemas for each table. Metadata should include source, artifact_type, chunk_id, text_content, and key fields such as `kind`, `api_path`, `record_name`, etc. Embeddings table should store a 768‑dimensional vector and reference the `chunk_id`.\n3. Update `src/janapada-memory/vector_index.py` (or equivalent) to write and read vectors from BigQuery instead of the current vector database. Use BigQuery’s vector search capabilities (`approximate_neighbors` functions) and document how to run migrations.\n4. Add configuration parameters (project_id, dataset_id, table names, credentials) to the project’s `.env` and provide defaults in `.env.example`.\n\n### Acceptance Criteria\n- A BigQuery dataset exists with three tables and the correct schemas.\n- Environment variables are defined for BigQuery access.\n- The vector index code writes and queries embeddings via BigQuery without errors.\n- Running `make setup` creates the BigQuery tables if they do not exist.\n", "labels": ["epic", "area:bigquery", "type:infra", "priority:P0"], "assignees": ["python_fastapi"], "milestone": "M1 Foundations"}
{"title": "M1: Parse and ingest Kubernetes, FastAPI, COBOL, IRS, and MUMPS artifacts", "body": "### Overview\nImplement data ingestion scripts that extract knowledge artifacts from multiple source types and write them into BigQuery. These parsers should chunk the data according to the strategies defined in our ingestion plan.\n\n### Tasks\n1. **Kubernetes & FastAPI (Backend AI 1)**:\n   - Use the Kubernetes Python client to fetch live `*.yaml` and `*.json` resources or clone relevant repos to read manifest files.\n   - For FastAPI projects, download or fetch the `/openapi.json` definition and inspect the `src/` directory for Pydantic models and route handlers via AST parsing.\n   - Chunk each deployment, service, or endpoint as a separate record, capturing metadata such as `kind`, `api_path`, `http_method`, and model names.\n2. **COBOL, IRS, MUMPS (Backend AI 2)**:\n   - Write parsers to read `.cpy` and `.cbl` copybooks, splitting on `01`‑level data structures, capturing field names and PIC clauses.\n   - Parse IRS IMF copybook definitions and extract record layouts with start positions, lengths, and types.\n   - Traverse MUMPS globals and FileMan dictionaries; create chunks per FileMan file and per top‑level global node. Capture file numbers, field numbers, and global names.\n3. Normalize all extracted chunks into a common schema and upsert them into the `source_metadata` table with source identifiers and artifact type tags.\n\n### Acceptance Criteria\n- Parsers for Kubernetes/FastAPI and COBOL/IRS/MUMPS source types run independently and write at least 100 sample rows into BigQuery.\n- Each row has the required metadata fields and a text content field.\n- Ingestion scripts can be invoked via `make ingest_k8s`, `make ingest_fastapi`, `make ingest_cobol`, etc.\n", "labels": ["epic", "area:data-ingestion", "type:feature", "priority:P0"], "assignees": ["backend_ai_1", "backend_ai_2"], "milestone": "M1 Foundations"}
{"title": "M2: Compute embeddings and populate BigQuery vector table", "body": "### Overview\nCompute 768‑dimensional embeddings for each ingested chunk using Gemini or Vertex AI and store them in BigQuery’s vector table. Ensure embedding generation is batched and cached to reduce costs.\n\n### Tasks\n1. Add a module in `pipeline/embedding.py` that iterates over rows in `source_metadata` lacking an embedding, calls the Vertex AI embedding endpoint (e.g., `text-embedding-004`), and collects the resulting vectors.\n2. Implement batching and exponential backoff for API calls and write a local disk cache keyed by a hash of the text content.\n3. Extend the BigQuery vector table creation script with a `VECTOR<768>` column. Insert new embeddings along with their `chunk_id` and source metadata.\n4. Document the embedding generation process in the repo’s README, including expected runtime and cost estimates.\n\n### Acceptance Criteria\n- Running `make embeddings` populates the vector table with embeddings for all existing metadata rows.\n- Re‑running the command does not duplicate existing embeddings (idempotent behaviour).\n- Embedding generation logs the number of API calls and caches repeated invocations.\n- At least one query using BigQuery’s `approximate_neighbors` function returns the nearest neighbours for a test chunk.\n", "labels": ["epic", "area:embedding", "type:feature", "priority:P0"], "assignees": ["backend_ai_2"], "milestone": "M2 Embeddings"}
{"title": "M2: Integrate BigQuery vector search into KonveyN2AI memory agent", "body": "### Overview\nModify the Janapada memory service to use BigQuery for vector similarity search. This will allow the existing orchestrator to query BigQuery instead of the previous vector database.\n\n### Tasks\n1. Refactor `src/janapada-memory` so that the `similarity_search` method invokes a BigQuery SQL statement using `approximate_neighbors` on the `source_embeddings` table.\n2. Update configuration loading to accept BigQuery credentials and dataset/table names. Use `google-cloud-bigquery` library for execution.\n3. Write a unit test that mocks BigQuery and verifies that queries return expected chunk IDs in order of similarity.\n4. Provide a fallback path: if BigQuery is unavailable, default to a local in‑memory vector index with approximate search for testing.\n\n### Acceptance Criteria\n- The memory agent returns search results from BigQuery when BigQuery is configured.\n- Unit tests cover BigQuery query construction and fallback logic.\n- The orchestrator (`svami`) can answer a simple knowledge question end‑to‑end using BigQuery as the backend store.\n", "labels": ["epic", "area:bigquery", "type:feature", "priority:P0"], "assignees": ["python_fastapi"], "milestone": "M2 Embeddings"}
{"title": "M3: Define coverage and confidence metrics for knowledge gaps", "body": "### Overview\nCreate a set of deterministic rules and scoring functions that determine when a knowledge artifact has insufficient documentation or metadata. These metrics will feed into the gap detection algorithm and allow us to compute heat maps and progress.\n\n### Tasks\n1. For each artifact type (Kubernetes resource, FastAPI endpoint, COBOL record, IRS record, MUMPS file), enumerate the required fields or descriptions that indicate adequate documentation. Examples: descriptions in YAML comments, docstrings for FastAPI routes, `PIC` comments in COBOL copybooks.\n2. Implement a rule evaluator that assigns a binary pass/fail for each rule and computes a confidence score between 0 and 1 based on factors such as field completeness, presence of owner tags, and adherence to naming standards.\n3. Write a BigQuery SQL script to join the rules with `source_metadata` and output a `gap_metrics` table with columns: `chunk_id`, `artifact_type`, `rule_name`, `passed`, `confidence`, `severity` (1–5), and `suggested_fix`.\n4. Collaborate with the backend team to ensure the rule definitions are transparent and easily adjustable via configuration files.\n\n### Acceptance Criteria\n- A documented set of rules exists for all artifact types.\n- Running `make compute_metrics` populates the `gap_metrics` table with at least one rule applied per record.\n- Confidence scores are calculated and stored.\n- The metrics calculation can be rerun without duplicating results.\n", "labels": ["epic", "area:gap-detection", "type:feature", "priority:P0"], "assignees": ["data_analytics", "backend_ai_1"], "milestone": "M3 Gap Detection"}
{"title": "M3: Generate heat maps and dashboards for gap visualization", "body": "### Overview\nCreate visualizations that display the distribution and severity of knowledge gaps across artifact types and sources. These heat maps will help the team and judges quickly understand where documentation is lacking.\n\n### Tasks\n1. Use BigQuery to aggregate `gap_metrics` by artifact_type and rule_name, producing summary tables with counts of passed vs failed rules and average confidence scores.\n2. Write a Python script (in `pipeline/visualization.py`) that reads the summary tables and generates heat maps using matplotlib. Rows should represent artifact types (Kubernetes, FastAPI, COBOL, IRS, MUMPS) and columns should represent gap categories (Missing Description, Missing Owner, Stale Doc, etc.), coloured by severity and gap count.\n3. Develop a dashboard using a lightweight framework (e.g., Plotly Dash or Streamlit) or embed static HTML/JS charts that display the heat map, progress over time, and high‑level metrics. Ensure the dashboard can be run locally or served via Cloud Run.\n4. Provide guidelines on how to embed the heat maps in the final notebook to comply with Kaggle presentation rules.\n\n### Acceptance Criteria\n- The `make visuals` command produces at least one heat map image and a summary CSV.\n- A minimal dashboard runs locally and displays the heat map and progress chart.\n- The heat map clearly shows areas with high gap severity and has a legend.\n- Instructions exist for including the visualizations in the Kaggle notebook or front‑end demo.\n", "labels": ["epic", "area:heatmap", "type:feature", "priority:P0"], "assignees": ["data_analytics", "frontend"], "milestone": "M3 Gap Detection"}
{"title": "M3: Implement progress tracking and reporting dashboards", "body": "### Overview\nTrack improvements in documentation coverage over time and provide a reporting interface for stakeholders to monitor progress. This includes capturing daily snapshots of metrics and visualizing trends.\n\n### Tasks\n1. Design a `progress` table in BigQuery that records daily snapshots of coverage metrics, including total chunks processed, total gaps detected, mean confidence scores, and severity distributions.\n2. Create a scheduled job (or a make target) that computes and inserts a new snapshot into the `progress` table each time metrics are recomputed.\n3. Extend the dashboard from the heat map task to include a line chart showing coverage percentage over time and a bar chart of gaps closed per rule.\n4. Write a report generator that outputs markdown/CSV summaries of progress for inclusion in the final submission.\n\n### Acceptance Criteria\n- Progress snapshots are recorded daily or on demand in the `progress` table.\n- The dashboard updates dynamically to reflect new snapshots.\n- The line chart clearly shows whether coverage is improving.\n- A script exists to export a summary report of progress.\n", "labels": ["epic", "area:dashboard", "type:feature", "priority:P1"], "assignees": ["data_analytics", "frontend"], "milestone": "M3 Gap Detection"}
{"title": "M4: Expose knowledge gap detection via Svami orchestrator", "body": "### Overview\nConnect the gap detection pipeline to the existing Svami orchestrator so that users can query gap analyses via a chat interface or API. This ties the new BigQuery‑powered memory and analytics into KonveyN2AI’s workflow.\n\n### Tasks\n1. Define a new RPC endpoint or route in `src/svami-orchestrator` (e.g., `get_gap_analysis`) that accepts parameters such as `artifact_type`, `rule`, and returns summary metrics and suggested fixes.\n2. In the orchestrator, call the memory agent to perform a semantic search on the requested topic and join the results with the `gap_metrics` and `suggested_fix` columns.\n3. Format the response into a user‑friendly message with bullet points outlining the top gaps and links to relevant code/doc locations.\n4. Add a notebook example demonstrating how to call this endpoint from within a Kaggle notebook and display the results inline.\n\n### Acceptance Criteria\n- A new API endpoint is documented and accessible via the orchestrator.\n- The orchestrator returns gap analysis results without error.\n- The notebook example successfully retrieves and displays gap summaries.\n- Existing tests are updated or extended to cover the new endpoint.\n", "labels": ["epic", "area:orchestrator", "type:feature", "priority:P1"], "assignees": ["backend_ai_1", "python_fastapi"], "milestone": "M4 Orchestrator & Demo"}
{"title": "M4: Build front‑end chat widget or enrich notebook outputs", "body": "### Overview\nCreate a simple front‑end to display knowledge gap results either as a chat UI (similar to the Vercel‑based frontend used previously) or as rich notebook outputs for the Kaggle presentation.\n\n### Tasks\n1. If opting for a chat UI: build a lightweight web page that connects to the Svami orchestrator via HTTP/WebSocket, allowing users to ask questions and see gap analyses in response. Ensure the UI renders heat maps and progress charts inline.\n2. If opting for notebook outputs: develop custom Jupyter widgets or use existing libraries (e.g., ipywidgets, Plotly) to render interactive heat maps and dashboards directly in the notebook.\n3. Work with the data analytics team to ensure the visualisations load quickly and are visually appealing.\n4. Ensure any external calls from the front‑end respect CORS and authentication requirements.\n\n### Acceptance Criteria\n- A working front‑end chat widget OR notebook widgets are available and documented.\n- Users can submit a query and receive gap analysis results along with heat maps within the UI.\n- The UI meets Kaggle’s presentation guidelines (no external network calls in the final notebook).\n", "labels": ["epic", "area:frontend", "type:feature", "priority:P1"], "assignees": ["frontend"], "milestone": "M4 Orchestrator & Demo"}
{"title": "M4: Prepare final README, instructions, and submission materials", "body": "### Overview\nWrite the documentation necessary for the Kaggle submission, including clear setup instructions, explanation of the BigQuery integration, and guidance on running the notebook and dashboards.\n\n### Tasks\n1. Update the repository README to include sections on data ingestion, embedding generation, gap detection, heat map visualization, and how to run the Svami orchestration demo.\n2. Create a `HACKATHON.md` or similar document that outlines the hackathon problem statement, our solution approach, and instructions for reproducing results on Kaggle.\n3. Provide a `requirements.txt`/`environment.yaml` specifically for the hackathon environment.\n4. Ensure all scripts and notebooks run end‑to‑end with no missing dependencies and produce outputs within the allowed time.\n\n### Acceptance Criteria\n- The README explains how to set up and run the project using BigQuery.\n- There is a clear, step‑by‑step guide for reproducing the gap analysis in a Kaggle notebook.\n- All dependencies are documented and installable via `pip` or `conda`.\n- The documentation has been reviewed by at least one team member and updated based on feedback.\n", "labels": ["epic", "area:docs", "type:infra", "priority:P0"], "assignees": ["python_fastapi", "data_analytics"], "milestone": "M4 Orchestrator & Demo"}
{"title": "M4: Create demo video and submission assets", "body": "### Overview\nRecord a 90‑second video showcasing the Semantic Gap Detector working end‑to‑end and prepare any additional assets required for the Kaggle submission.\n\n### Tasks\n1. Draft a script highlighting the problem (missing documentation across heterogeneous sources), our BigQuery‑powered ingestion and embedding pipeline, the gap metrics and heat maps, and a short demo of the Svami chat interface or notebook.\n2. Record screen captures of the ingestion process, running the notebook, viewing the dashboard, and interacting with the chat UI (if applicable).\n3. Edit the footage into a polished 90‑second video with voiceover or captions.\n4. Compress the video according to Kaggle upload requirements and add it to the repository under `assets/demo.mp4`.\n\n### Acceptance Criteria\n- A finished video file under 90 seconds is available in the repo.\n- The video clearly explains the problem, our solution, and shows working functionality.\n- All visual assets used in the video are checked into the repository.\n", "labels": ["epic", "area:demo", "type:infra", "priority:P1"], "assignees": ["frontend", "data_analytics"], "milestone": "M4 Orchestrator & Demo"}
{"title": "M5: Evaluate gap detection precision and finalize metrics", "body": "### Overview\nPerform a small evaluation of the gap detection rules and confidence scoring to estimate precision and recall on a labelled sample. Iterate on the rule definitions if necessary to improve precision.\n\n### Tasks\n1. Draw a sample of 50–100 chunks across the various artifact types and manually label whether each rule’s gap classification is correct.\n2. Compute precision and recall metrics for each rule and the overall system.\n3. Analyse false positives and false negatives to refine rule thresholds or adjust confidence scoring.\n4. Document the evaluation methodology and results in a report (`eval_report.md`) and include a summary in the README.\n\n### Acceptance Criteria\n- A labelled dataset and evaluation script are stored under `/eval`.\n- Precision and recall are computed and included in `eval_report.md`.\n- Adjustments to rules or scoring are implemented if precision is below 0.8.\n- The evaluation results are referenced in the final README and hackathon report.\n", "labels": ["epic", "area:eval", "type:feature", "priority:P1"], "assignees": ["data_analytics", "backend_ai_1"], "milestone": "M5 Submission"}
