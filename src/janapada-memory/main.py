"""
Janapada Memory Service - Semantic search agent using Vertex AI embeddings and Matching Engine.

This service implements the memory component of the KonveyN2AI three-tier architecture,
providing semantic search capabilities for code snippets using Google Cloud Vertex AI.

Implements real Vertex AI embeddings with textembedding-gecko-001 model.
"""

import logging
import os
from typing import Any, Optional, List
import asyncio
from functools import lru_cache

from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse

# Google Cloud Vertex AI imports
try:
    import vertexai
    from vertexai.language_models import TextEmbeddingModel

    VERTEX_AI_AVAILABLE = True
except ImportError:
    VERTEX_AI_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("Vertex AI not available - install google-cloud-aiplatform")

# Import common models and JSON-RPC infrastructure
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from common.models import SearchRequest, Snippet, JsonRpcError, JsonRpcErrorCode
from common.rpc_server import JsonRpcServer

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize FastAPI application
app = FastAPI(
    title="Janapada Memory Service",
    description="Semantic search agent using Vertex AI embeddings and Matching Engine",
    version="1.0.0",
)

# Initialize JSON-RPC server
rpc_server = JsonRpcServer(
    title="Janapada Memory Service",
    version="1.0.0",
    description="Semantic search agent using Vertex AI embeddings and Matching Engine",
)

# Global variables for caching (will be initialized in startup)
embedding_model = None
matching_engine_index = None
faiss_index = None  # Fallback index


@app.on_event("startup")
async def startup_event():
    """Initialize Vertex AI components on startup."""
    global embedding_model, matching_engine_index

    logger.info("Starting Janapada Memory Service...")

    # Initialize Vertex AI Embeddings (Subtask 6.2)
    try:
        if VERTEX_AI_AVAILABLE:
            # Initialize Vertex AI with project and location
            project_id = os.environ.get("GOOGLE_CLOUD_PROJECT", "konveyn2ai")
            location = os.environ.get("GOOGLE_CLOUD_LOCATION", "us-central1")

            logger.info(
                f"Initializing Vertex AI: project={project_id}, location={location}"
            )
            vertexai.init(project=project_id, location=location)

            # Load the text embedding model
            embedding_model = TextEmbeddingModel.from_pretrained(
                "textembedding-gecko-001"
            )
            logger.info("‚úÖ Vertex AI TextEmbeddingModel initialized successfully")
        else:
            logger.warning("‚ö†Ô∏è Vertex AI not available - using fallback mode")
            embedding_model = None
    except Exception as e:
        logger.error(f"‚ùå Vertex AI initialization failed: {e}")
        embedding_model = None

    # Initialize Matching Engine (will be implemented in subtask 6.3)
    try:
        # TODO: Initialize Matching Engine in subtask 6.3
        # index_endpoint = aiplatform.MatchingEngineIndexEndpoint(os.environ["INDEX_ENDPOINT_ID"])
        # matching_engine_index = index_endpoint.get_index(os.environ["INDEX_ID"])
        logger.info(
            "Matching Engine initialization placeholder - will be implemented in subtask 6.3"
        )
    except Exception as e:
        logger.warning(f"Matching Engine initialization failed: {e}")

    # Initialize FAISS fallback (will be implemented in subtask 6.4)
    try:
        # TODO: Initialize FAISS fallback in subtask 6.4
        logger.info(
            "FAISS fallback initialization placeholder - will be implemented in subtask 6.4"
        )
    except Exception as e:
        logger.warning(f"FAISS initialization failed: {e}")

    logger.info("Janapada Memory Service startup complete")


# Embedding generation with caching
@lru_cache(maxsize=1000)
def generate_embedding_cached(text: str) -> Optional[List[float]]:
    """
    Generate embedding for text with LRU caching for performance.

    Adapted from konveyor/core/search/embeddings/azure_openai.py:60-94
    Platform migration: Azure OpenAI ‚Üí Vertex AI TextEmbeddingModel

    Args:
        text: Input text to embed

    Returns:
        List of floats representing the embedding vector (3072 dimensions)
        None if embedding generation fails
    """
    if not embedding_model:
        logger.warning("Embedding model not available - cannot generate embeddings")
        return None

    if not text or not text.strip():
        logger.warning("Empty text provided for embedding generation")
        return None

    try:
        # Generate embedding using Vertex AI TextEmbeddingModel
        # textembedding-gecko-001 produces 3072-dimensional embeddings
        embeddings = embedding_model.get_embeddings([text])

        if embeddings and len(embeddings) > 0:
            embedding_vector = embeddings[0].values
            logger.debug(f"Generated embedding: {len(embedding_vector)} dimensions")
            return embedding_vector
        else:
            logger.warning("No embeddings returned from Vertex AI")
            return None

    except Exception as e:
        logger.error(f"Embedding generation failed: {str(e)}")
        return None


def search_with_embeddings(query: str, k: int) -> List[Snippet]:
    """
    Search for relevant code snippets using embedding-based similarity.

    This is a simplified implementation for subtask 6.2.
    Full vector search with Matching Engine will be implemented in subtask 6.3.

    Args:
        query: Search query text
        k: Number of results to return

    Returns:
        List of relevant code snippets
    """
    # Generate embedding for the query
    query_embedding = generate_embedding_cached(query)

    if not query_embedding:
        logger.warning("Could not generate query embedding - falling back to mock data")
        return get_mock_snippets(query, k)

    logger.info(f"Generated query embedding: {len(query_embedding)} dimensions")

    # For subtask 6.2, we demonstrate embedding generation but still return
    # enhanced mock data that shows the embedding was generated successfully
    # Real vector search will be implemented in subtask 6.3 with Matching Engine

    enhanced_snippets = [
        Snippet(
            file_path="src/auth/middleware.py",
            content=f"# Query embedding generated: {len(query_embedding)} dims\ndef authenticate_request(token: str) -> bool:\n    return validate_token(token)",
        ),
        Snippet(
            file_path="src/auth/tokens.py",
            content="def validate_token(token: str) -> bool:\n    # Token validation logic\n    return jwt.decode(token, SECRET_KEY)",
        ),
        Snippet(
            file_path="src/search/embeddings.py",
            content=f"# Vertex AI embedding for '{query[:30]}...'\n# Dimensions: {len(query_embedding)}\ndef generate_embeddings(text: str):\n    return model.get_embeddings([text])",
        ),
    ]

    return enhanced_snippets[:k]


def get_mock_snippets(query: str, k: int) -> List[Snippet]:
    """Fallback mock snippets when embedding generation fails."""
    mock_snippets = [
        Snippet(
            file_path="src/auth/middleware.py",
            content="def authenticate_request(token: str) -> bool:\n    return validate_token(token)",
        ),
        Snippet(
            file_path="src/auth/tokens.py",
            content="def validate_token(token: str) -> bool:\n    # Token validation logic\n    return True",
        ),
    ]
    return mock_snippets[:k]


@rpc_server.method("search", "Search for relevant code snippets using semantic search")
def search_snippets(
    query: str, k: int = 5, request_id: Optional[str] = None
) -> dict[str, Any]:
    """
    Search for relevant code snippets using semantic search with Vertex AI embeddings.

    Implements real embedding generation with textembedding-gecko-001 model.
    Platform migration: Azure AI Search ‚Üí Vertex AI Embeddings (Subtask 6.2)

    Args:
        query: Search query text
        k: Number of results to return (1-20)
        request_id: Optional request ID for tracing

    Returns:
        Dictionary containing list of snippets with file_path and content

    Raises:
        ValueError: If search fails or parameters are invalid
    """
    logger.info(
        f"üîç Search request: query='{query[:50]}...', k={k}, request_id={request_id}"
    )

    # Validate parameters
    if not query or not query.strip():
        raise ValueError("Query parameter cannot be empty")

    if k < 1 or k > 20:
        raise ValueError(f"Parameter k must be between 1 and 20, got {k}")

    try:
        # Use real embedding-based search (Subtask 6.2 implementation)
        result_snippets = search_with_embeddings(query, k)

        logger.info(f"‚úÖ Search completed: returned {len(result_snippets)} snippets")

        return {"snippets": [snippet.model_dump() for snippet in result_snippets]}

    except Exception as e:
        logger.error(f"‚ùå Search failed: {str(e)}")
        raise RuntimeError(f"Search operation failed: {str(e)}")


# JSON-RPC endpoint
@app.post("/")
async def json_rpc_endpoint(request: Request):
    """Handle JSON-RPC requests."""
    return await rpc_server.handle_request(request)


@app.get("/health")
async def health_check():
    """Health check endpoint with detailed component status."""
    # Check if core components are available
    embedding_status = "ready" if embedding_model is not None else "unavailable"
    if not VERTEX_AI_AVAILABLE:
        embedding_status = "not_installed"

    health_status = {
        "status": "healthy",
        "service": "janapada-memory",
        "version": "1.0.0",
        "components": {
            "vertex_ai_available": VERTEX_AI_AVAILABLE,
            "embedding_model": embedding_status,
            "embedding_dimensions": 3072 if embedding_model else None,
            "embedding_model_name": "textembedding-gecko-001"
            if embedding_model
            else None,
            "matching_engine": "placeholder"
            if matching_engine_index is None
            else "ready",
            "faiss_fallback": "placeholder" if faiss_index is None else "ready",
        },
        "features": {
            "embedding_generation": embedding_model is not None,
            "embedding_caching": True,
            "vector_search": False,  # Will be True in subtask 6.3
            "fallback_search": True,
        },
    }

    # Determine overall status based on critical components
    if embedding_model is None and VERTEX_AI_AVAILABLE:
        health_status["status"] = "degraded"
    elif not VERTEX_AI_AVAILABLE:
        health_status["status"] = "limited"  # Can still work with fallback

    return health_status


@app.get("/.well-known/agent.json")
async def agent_manifest():
    """Agent manifest endpoint for service discovery."""
    return rpc_server.get_manifest()


@app.post("/debug/embedding")
async def debug_embedding(request: dict):
    """
    Debug endpoint to test embedding generation directly.

    This endpoint is for testing purposes and should be removed in production.
    """
    text = request.get("text", "")
    if not text:
        return {"error": "Text parameter required"}

    embedding = generate_embedding_cached(text)

    return {
        "text": text,
        "embedding_available": embedding is not None,
        "embedding_dimensions": len(embedding) if embedding else None,
        "embedding_preview": embedding[:5] if embedding else None,  # First 5 values
        "model_status": "ready" if embedding_model else "unavailable",
    }


# Add middleware for logging
rpc_server.add_middleware(
    lambda rpc_request, http_request: logger.info(
        f"JSON-RPC call: {rpc_request.method} (id: {rpc_request.id})"
    )
)

if __name__ == "__main__":
    import uvicorn

    port = int(os.environ.get("PORT", 8081))
    uvicorn.run(app, host="0.0.0.0", port=port)  # nosec B104
